# 音声処理ボトルネック解消とCPU負荷対策プラン

音声通話時に「データ流入速度 > 処理速度」となり、バッファ崩壊やCPU 100%が発生する問題を解決するための具体的な対策フローです。

## 原因の仮説
Discordからの音声パケット（Opus/PCM）はリアルタイムで絶え間なく送られてきますが、サーバー側の処理（下記のいずれか）が追いついていません。
1.  **VAD（発話検出）**: 無音やノイズまで処理に回している。
2.  **Whisper推論**: 音声の長さよりも推論時間がかかっている（RTF > 1）。
3.  **バッファリング**: 処理待ちのキューが無制限に溜まり続け、メモリとGCを圧迫、最終的にクラッシュする。

---

## 対策フローチャート

### Phase 1: 現状把握と計測 (Measurement)
まずは「どこで詰まっているか」を数値化します。
- [ ] **Real Time Factor (RTF) のログ出力**: `処理時間 / 音声の長さ` をログに出す。これが `1.0` を超えていたら破綻している証拠です。
- [ ] **キューサイズの監視**: `discordVoice.ts` や `whisper_server.py` で、現在処理待ちのタスク数をログに出す。

### Phase 2: 入力の間引き (Input Throttling)
処理するデータを物理的に減らします。
- [ ] **VAD（無音検出）の厳格化**: ノイズゲートの閾値を上げる、無音判定時間を短くする。
- [ ] **Discordの声だけ拾う**: 複数人が喋っている場合、メインの話者以外を捨てる（または混ぜる前に捨てる）。
- [ ] **Backpressure（背圧制御）の実装**:
    - キューに未処理の音声が `N個` 以上溜まったら、新しい音声は**容赦なく捨てる（Drop）**。
    - 古い未処理データを破棄して、最新のデータを優先する（LIFO的アプローチ）。

### Phase 3: 処理の高速化 (Optimization)
処理能力自体を上げます。
- [ ] **モデルの軽量化**: `faster-whisper` のモデルを `small` → `base` や `tiny` に下げる、または `int8` 量子化を強制する。
- [ ] **パラメータ調整**: `beam_size=1` (greedy search) にして精度より速度を優先する。
- [ ] **GPUオフロードの確認**: 本当にGPUが使われているか（CPU処理になっていないか）再確認。

### Phase 4: 非同期処理の分離 (Architecture)
- [ ] **Worker Thread**: 音声受信ループと認識ループを完全に分離し、受信ループがブロックされないようにする（現状のPythonサーバー構成ならある程度分離できているはずだが、Client側の送信処理も確認）。

---

## 具体的な実装案（discordVoice.ts / whisper_server.py）

### 1. バッファ制限機能の追加
`discordVoice.ts` にて、サーバーに送信する前のキューサイズを監視します。

```typescript
// 擬似コード
if (processingQueue.length > 5) {
    console.warn("Processing too slow! Dropping audio chunk.");
    return; // 処理せず捨てる
}
```

### 2. Whisperサーバーのタイムアウト設定
推論があまりに長い場合（幻覚で無限ループしている場合など）、強制的に打ち切るタイムアウトを短く設定します。

```python
# whisper_server.py
segments, info = model.transcribe(
    ...,
    timeout=5.0 # 5秒以上かかったら諦めるなど（faster-whisperには直接ないが、スレッド制御等で実装）
)
```

## 推奨手順
1. **Phase 1（ログ出力）** を実装し、CPU 100%時のログを確認。
2. 明らかにRTFが悪いなら **Phase 3（モデル軽量化）**。
3. 処理速度は正常だがリクエスト過多なら **Phase 2（間引き・ドロップ）** を実装。
